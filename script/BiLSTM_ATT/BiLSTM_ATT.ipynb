{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM_ATT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZzVb3E8rMQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coding:utf8\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1)\n",
        "\n",
        "class BiLSTM_ATT(nn.Module):\n",
        "    def __init__(self,config,embedding_pre):\n",
        "        super(BiLSTM_ATT,self).__init__()\n",
        "        self.batch = config['BATCH']\n",
        "        \n",
        "        self.embedding_size = config['EMBEDDING_SIZE']\n",
        "        self.embedding_dim = config['EMBEDDING_DIM']\n",
        "        \n",
        "        self.hidden_dim = config['HIDDEN_DIM']\n",
        "        self.tag_size = config['TAG_SIZE']\n",
        "        \n",
        "        self.pos_size = config['POS_SIZE']\n",
        "        self.pos_dim = config['POS_DIM']\n",
        "        \n",
        "        self.pretrained = config['pretrained']\n",
        "        if self.pretrained:\n",
        "            #self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_pre))\n",
        "            self.word_embeds = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_pre),freeze=False)\n",
        "        else:\n",
        "            self.word_embeds = nn.Embedding(self.embedding_size,self.embedding_dim)\n",
        "        \n",
        "        self.pos1_embeds = nn.Embedding(self.pos_size,self.pos_dim)\n",
        "        self.pos2_embeds = nn.Embedding(self.pos_size,self.pos_dim)\n",
        "        self.relation_embeds = nn.Embedding(self.tag_size,self.hidden_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim+self.pos_dim*2,hidden_size=self.hidden_dim//2,num_layers=1, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(self.hidden_dim,self.tag_size)\n",
        "        \n",
        "        self.dropout_emb=nn.Dropout(p=0.5)\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        self.dropout_att=nn.Dropout(p=0.5)\n",
        "        \n",
        "        self.hidden = self.init_hidden()\n",
        "        \n",
        "        self.att_weight = nn.Parameter(torch.randn(self.batch,1,self.hidden_dim))\n",
        "        self.relation_bias = nn.Parameter(torch.randn(self.batch,self.tag_size,1))\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return torch.randn(2, self.batch, self.hidden_dim // 2)\n",
        "        \n",
        "    def init_hidden_lstm(self):\n",
        "        return (torch.randn(2, self.batch, self.hidden_dim // 2),\n",
        "                torch.randn(2, self.batch, self.hidden_dim // 2))\n",
        "                \n",
        "    def attention(self,H):\n",
        "        M = F.tanh(H)\n",
        "        a = F.softmax(torch.bmm(self.att_weight,M),2)\n",
        "        a = torch.transpose(a,1,2)\n",
        "        return torch.bmm(H,a)\n",
        "        \n",
        "    \n",
        "                \n",
        "    def forward(self,sentence,pos1,pos2):\n",
        "\n",
        "        self.hidden = self.init_hidden_lstm()\n",
        "\n",
        "        embeds = torch.cat((self.word_embeds(sentence),self.pos1_embeds(pos1),self.pos2_embeds(pos2)),2)\n",
        "        \n",
        "        embeds = torch.transpose(embeds,0,1)\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        \n",
        "        lstm_out = torch.transpose(lstm_out,0,1)\n",
        "        lstm_out = torch.transpose(lstm_out,1,2)\n",
        "        \n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        att_out = F.tanh(self.attention(lstm_out))\n",
        "        #att_out = self.dropout_att(att_out)\n",
        "        \n",
        "        relation = torch.tensor([i for i in range(self.tag_size)],dtype = torch.long).repeat(self.batch, 1)\n",
        "\n",
        "        relation = self.relation_embeds(relation)\n",
        "        \n",
        "        res = torch.add(torch.bmm(relation,att_out),self.relation_bias)\n",
        "        \n",
        "        res = F.softmax(res,1)\n",
        "\n",
        "        \n",
        "        return res.view(self.batch,-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjuv2SG0rX3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5e9dfe25-4115-4c70-aa08-e01bd0c8645f"
      },
      "source": [
        "#coding:utf8\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import codecs\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "with open('people_relation_train.pkl', 'rb') as inp:\n",
        "    word2id = pickle.load(inp)\n",
        "    id2word = pickle.load(inp)\n",
        "    relation2id = pickle.load(inp)\n",
        "    train = pickle.load(inp)\n",
        "    labels = pickle.load(inp)\n",
        "    position1 = pickle.load(inp)\n",
        "    position2 = pickle.load(inp)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsaAfeWKrqrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49844b0a-bfb7-4a9e-b48a-6dae78f37e82"
      },
      "source": [
        "print(inp)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_io.BufferedReader name='people_relation_train.pkl'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF78HA56uI59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2580da6e-d5a1-427b-df11-0e1e7a53b07f"
      },
      "source": [
        "print(position2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[15 16 17 ... 62 63 64]\n",
            " [25 26 27 ... 72 81 81]\n",
            " [ 9 10 11 ... 56 57 58]\n",
            " ...\n",
            " [27 28 29 ... 81 81 81]\n",
            " [32 33 34 ... 81 81 81]\n",
            " [18 19 20 ... 65 66 67]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns7-jDmpuM7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('people_relation_test.pkl', 'rb') as inp:\n",
        "    test = pickle.load(inp)\n",
        "    labels_t = pickle.load(inp)\n",
        "    position1_t = pickle.load(inp)\n",
        "    position2_t = pickle.load(inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO6QgO9Iuvkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "caa7116e-d2ca-4658-c93e-f853b8f428c2"
      },
      "source": [
        "print(\"train len\", len(train))\n",
        "print(\"test len\", len(test))\n",
        "print(\"word2id len\",len(word2id))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.utils.data as D\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train len 18000\n",
            "test len 3600\n",
            "word2id len 4408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X56UX6gquyI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = len(word2id)+1        \n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "POS_SIZE = 82  #不同数据集这里可能会报错。\n",
        "POS_DIM = 25\n",
        "\n",
        "HIDDEN_DIM = 200\n",
        "\n",
        "TAG_SIZE = len(relation2id)\n",
        "\n",
        "BATCH = 128\n",
        "EPOCHS = 100\n",
        "\n",
        "config={}\n",
        "config['EMBEDDING_SIZE'] = EMBEDDING_SIZE\n",
        "config['EMBEDDING_DIM'] = EMBEDDING_DIM\n",
        "config['POS_SIZE'] = POS_SIZE\n",
        "config['POS_DIM'] = POS_DIM\n",
        "config['HIDDEN_DIM'] = HIDDEN_DIM\n",
        "config['TAG_SIZE'] = TAG_SIZE\n",
        "config['BATCH'] = BATCH\n",
        "config[\"pretrained\"]=False\n",
        "\n",
        "learning_rate = 0.0005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPya-Prtu90N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "928ee181-afef-4769-e967-01320f566474"
      },
      "source": [
        "embedding_pre = []\n",
        "if len(sys.argv)==2 and sys.argv[1]==\"pretrained\":\n",
        "    print(\"use pretrained embedding\")\n",
        "    config[\"pretrained\"]=True\n",
        "    word2vec = {}\n",
        "    with codecs.open('vec.txt','r','utf-8') as input_data:   \n",
        "        for line in input_data.readlines():\n",
        "            word2vec[line.split()[0]] = map(eval,line.split()[1:])\n",
        "\n",
        "    unknow_pre = []\n",
        "    unknow_pre.extend([1]*100)\n",
        "    embedding_pre.append(unknow_pre) #wordvec id 0\n",
        "    for word in word2id:\n",
        "        if word2vec.has_key(word):\n",
        "            embedding_pre.append(word2vec[word])\n",
        "        else:\n",
        "            embedding_pre.append(unknow_pre)\n",
        "\n",
        "    embedding_pre = np.asarray(embedding_pre)\n",
        "    print(embedding_pre.shape)\n",
        "\n",
        "model = BiLSTM_ATT(config,embedding_pre)\n",
        "#model = torch.load('model/model_epoch20.pkl')\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss(size_average=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4trW5ReHvYfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = torch.LongTensor(train[:len(train)-len(train)%BATCH])\n",
        "position1 = torch.LongTensor(position1[:len(train)-len(train)%BATCH])\n",
        "position2 = torch.LongTensor(position2[:len(train)-len(train)%BATCH])\n",
        "labels = torch.LongTensor(labels[:len(train)-len(train)%BATCH])\n",
        "train_datasets = D.TensorDataset(train,position1,position2,labels)\n",
        "train_dataloader = D.DataLoader(train_datasets,BATCH,True,num_workers=2)\n",
        "\n",
        "\n",
        "test = torch.LongTensor(test[:len(test)-len(test)%BATCH])\n",
        "position1_t = torch.LongTensor(position1_t[:len(test)-len(test)%BATCH])\n",
        "position2_t = torch.LongTensor(position2_t[:len(test)-len(test)%BATCH])\n",
        "labels_t = torch.LongTensor(labels_t[:len(test)-len(test)%BATCH])\n",
        "test_datasets = D.TensorDataset(test,position1_t,position2_t,labels_t)\n",
        "test_dataloader = D.DataLoader(test_datasets,BATCH,True,num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCY3NWnyvi5R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dca91cc4-8836-4aca-c84f-b1de22cb8ef6"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(\"epoch:\",epoch)\n",
        "    acc=0\n",
        "    total=0\n",
        "    \n",
        "    for sentence,pos1,pos2,tag in train_dataloader:\n",
        "        sentence = Variable(sentence)\n",
        "        pos1 = Variable(pos1)\n",
        "        pos2 = Variable(pos2)\n",
        "        y = model(sentence,pos1,pos2)  \n",
        "        tags = Variable(tag)\n",
        "        loss = criterion(y, tags)      \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()    \n",
        "       \n",
        "        y = np.argmax(y.data.numpy(),axis=1)\n",
        "\n",
        "        for y1,y2 in zip(y,tag):\n",
        "            if y1==y2:\n",
        "                acc+=1\n",
        "            total+=1\n",
        "        \n",
        "    print(\"train:\",100*float(acc)/total,\"%\")\n",
        "      \n",
        "    acc_t=0\n",
        "    total_t=0\n",
        "    count_predict = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    count_total = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    count_right = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    for sentence,pos1,pos2,tag in test_dataloader:\n",
        "        sentence = Variable(sentence)\n",
        "        pos1 = Variable(pos1)\n",
        "        pos2 = Variable(pos2)\n",
        "        y = model(sentence,pos1,pos2)\n",
        "        y = np.argmax(y.data.numpy(),axis=1)\n",
        "        for y1,y2 in zip(y,tag):\n",
        "            count_predict[y1]+=1\n",
        "            count_total[y2]+=1\n",
        "            if y1==y2:\n",
        "                count_right[y1]+=1\n",
        "\n",
        "    \n",
        "    precision = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    recall = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    for i in range(len(count_predict)):\n",
        "        if count_predict[i]!=0 :\n",
        "            precision[i] = float(count_right[i])/count_predict[i]\n",
        "            \n",
        "        if count_total[i]!=0:\n",
        "            recall[i] = float(count_right[i])/count_total[i]\n",
        "    \n",
        "\n",
        "    precision = sum(precision)/len(relation2id)\n",
        "    recall = sum(recall)/len(relation2id)    \n",
        "    print(\"准确率：\",precision)\n",
        "    print(\"召回率：\",recall)\n",
        "    print(\"f：\", (2*precision*recall)/(precision+recall))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train: 10.083705357142858 %\n",
            "准确率： 0.10135601441257797\n",
            "召回率： 0.10945618153364632\n",
            "f： 0.10525047911269769\n",
            "epoch: 1\n",
            "train: 12.544642857142858 %\n",
            "准确率： 0.1386137037163615\n",
            "召回率： 0.13963223787167447\n",
            "f： 0.1391211065946345\n",
            "epoch: 2\n",
            "train: 15.424107142857142 %\n",
            "准确率： 0.14558732799136237\n",
            "召回率： 0.1609311424100157\n",
            "f： 0.15287519204562927\n",
            "epoch: 3\n",
            "train: 17.946428571428573 %\n",
            "准确率： 0.15886324294789253\n",
            "召回率： 0.1827347417840376\n",
            "f： 0.16996490012573817\n",
            "epoch: 4\n",
            "train: 21.255580357142858 %\n",
            "准确率： 0.19659774208835226\n",
            "召回率： 0.21242957746478872\n",
            "f： 0.20420726580310627\n",
            "epoch: 5\n",
            "train: 25.8984375 %\n",
            "准确率： 0.24346088164439125\n",
            "召回率： 0.2612284820031299\n",
            "f： 0.2520319274393346\n",
            "epoch: 6\n",
            "train: 28.565848214285715 %\n",
            "准确率： 0.26852098163132715\n",
            "召回率： 0.2872965571205008\n",
            "f： 0.27759164890887966\n",
            "epoch: 7\n",
            "train: 31.456473214285715 %\n",
            "准确率： 0.29971077039001626\n",
            "召回率： 0.3138223787167449\n",
            "f： 0.30660428707970705\n",
            "epoch: 8\n",
            "train: 34.2578125 %\n",
            "准确率： 0.3224151874902961\n",
            "召回率： 0.3350508607198748\n",
            "f： 0.3286116032055581\n",
            "epoch: 9\n",
            "train: 36.166294642857146 %\n",
            "准确率： 0.32515781558086815\n",
            "召回率： 0.3561932707355242\n",
            "f： 0.3399687126445296\n",
            "epoch: 10\n",
            "train: 38.777901785714285 %\n",
            "准确率： 0.33680561231738354\n",
            "召回率： 0.35177230046948355\n",
            "f： 0.34412630104965725\n",
            "epoch: 11\n",
            "train: 40.184151785714285 %\n",
            "准确率： 0.3651174179044876\n",
            "召回率： 0.37348200312989044\n",
            "f： 0.3692523463546941\n",
            "epoch: 12\n",
            "train: 42.416294642857146 %\n",
            "准确率： 0.3781732701491119\n",
            "召回率： 0.3967527386541471\n",
            "f： 0.3872402756210698\n",
            "epoch: 13\n",
            "train: 44.31361607142857 %\n",
            "准确率： 0.3956663007770274\n",
            "召回率： 0.39977699530516425\n",
            "f： 0.39771102641062456\n",
            "epoch: 14\n",
            "train: 45.7421875 %\n",
            "准确率： 0.39757686085657723\n",
            "召回率： 0.4038341158059468\n",
            "f： 0.40068106064020115\n",
            "epoch: 15\n",
            "train: 48.052455357142854 %\n",
            "准确率： 0.4137464719341033\n",
            "召回率： 0.4305672926447574\n",
            "f： 0.4219893260909825\n",
            "epoch: 16\n",
            "train: 49.017857142857146 %\n",
            "准确率： 0.42781155890192585\n",
            "召回率： 0.43670187793427234\n",
            "f： 0.432211006131205\n",
            "epoch: 17\n",
            "train: 50.84263392857143 %\n",
            "准确率： 0.4466517477006398\n",
            "召回率： 0.4479890453834115\n",
            "f： 0.44731939705423895\n",
            "epoch: 18\n",
            "train: 51.03794642857143 %\n",
            "准确率： 0.4475592330718056\n",
            "召回率： 0.4527621283255086\n",
            "f： 0.4501456471116616\n",
            "epoch: 19\n",
            "train: 53.28125 %\n",
            "准确率： 0.45257069739646405\n",
            "召回率： 0.46433098591549293\n",
            "f： 0.45837542223611705\n",
            "epoch: 20\n",
            "train: 54.4921875 %\n",
            "准确率： 0.45224134782983333\n",
            "召回率： 0.4622417840375586\n",
            "f： 0.4571868854694754\n",
            "epoch: 21\n",
            "train: 54.55357142857143 %\n",
            "准确率： 0.46424427482486125\n",
            "召回率： 0.4736424100156494\n",
            "f： 0.46889625520466094\n",
            "epoch: 22\n",
            "train: 55.95982142857143 %\n",
            "准确率： 0.4707922632045131\n",
            "召回率： 0.47665492957746475\n",
            "f： 0.47370545772465356\n",
            "epoch: 23\n",
            "train: 56.640625 %\n",
            "准确率： 0.4714085251480342\n",
            "召回率： 0.484745696400626\n",
            "f： 0.4779840922355776\n",
            "epoch: 24\n",
            "train: 57.885044642857146 %\n",
            "准确率： 0.48699480924822386\n",
            "召回率： 0.4977230046948356\n",
            "f： 0.492300466789017\n",
            "epoch: 25\n",
            "train: 58.88950892857143 %\n",
            "准确率： 0.4833802725214911\n",
            "召回率： 0.50037558685446\n",
            "f： 0.49173112461102303\n",
            "epoch: 26\n",
            "train: 59.464285714285715 %\n",
            "准确率： 0.48704108923293576\n",
            "召回率： 0.5012402190923319\n",
            "f： 0.49403865117670986\n",
            "epoch: 27\n",
            "train: 60.167410714285715 %\n",
            "准确率： 0.5036653515156936\n",
            "召回率： 0.5095931142410016\n",
            "f： 0.5066118935853069\n",
            "epoch: 28\n",
            "train: 60.708705357142854 %\n",
            "准确率： 0.4960296721124753\n",
            "召回率： 0.5031846635367762\n",
            "f： 0.4995815507470602\n",
            "epoch: 29\n",
            "train: 61.411830357142854 %\n",
            "准确率： 0.5015942922364237\n",
            "召回率： 0.5077347417840375\n",
            "f： 0.5046458387004943\n",
            "epoch: 30\n",
            "train: 62.779017857142854 %\n",
            "准确率： 0.5242606776599633\n",
            "召回率： 0.5121048513302034\n",
            "f： 0.5181114749212671\n",
            "epoch: 31\n",
            "train: 63.236607142857146 %\n",
            "准确率： 0.5169023075587703\n",
            "召回率： 0.5108059467918623\n",
            "f： 0.513836045382919\n",
            "epoch: 32\n",
            "train: 63.45982142857143 %\n",
            "准确率： 0.5168039411861546\n",
            "召回率： 0.5166862284820031\n",
            "f： 0.5167450781304442\n",
            "epoch: 33\n",
            "train: 62.39955357142857 %\n",
            "准确率： 0.5112241357369164\n",
            "召回率： 0.5154538341158059\n",
            "f： 0.5133302721902834\n",
            "epoch: 34\n",
            "train: 64.296875 %\n",
            "准确率： 0.5135323273456086\n",
            "召回率： 0.5217527386541472\n",
            "f： 0.5176098969828518\n",
            "epoch: 35\n",
            "train: 65.23995535714286 %\n",
            "准确率： 0.531915518067917\n",
            "召回率： 0.5390845070422534\n",
            "f： 0.5354760189034935\n",
            "epoch: 36\n",
            "train: 65.52455357142857 %\n",
            "准确率： 0.5180487423095178\n",
            "召回率： 0.5234350547730829\n",
            "f： 0.5207279701623682\n",
            "epoch: 37\n",
            "train: 66.27790178571429 %\n",
            "准确率： 0.5266191117079599\n",
            "召回率： 0.5196987480438184\n",
            "f： 0.5231360441759092\n",
            "epoch: 38\n",
            "train: 66.62946428571429 %\n",
            "准确率： 0.5319971779148679\n",
            "召回率： 0.5351017214397497\n",
            "f： 0.5335449336055071\n",
            "epoch: 39\n",
            "train: 67.421875 %\n",
            "准确率： 0.5436424758573497\n",
            "召回率： 0.5446400625978091\n",
            "f： 0.5441408120028838\n",
            "epoch: 40\n",
            "train: 67.80133928571429 %\n",
            "准确率： 0.522461754437611\n",
            "召回率： 0.5293035993740219\n",
            "f： 0.5258604234431196\n",
            "epoch: 41\n",
            "train: 67.89620535714286 %\n",
            "准确率： 0.5304631912830651\n",
            "召回率： 0.5358724569640064\n",
            "f： 0.5331541041680719\n",
            "epoch: 42\n",
            "train: 68.79464285714286 %\n",
            "准确率： 0.5315381609027163\n",
            "召回率： 0.530641627543036\n",
            "f： 0.5310895158631702\n",
            "epoch: 43\n",
            "train: 68.60491071428571 %\n",
            "准确率： 0.5428976966868949\n",
            "召回率： 0.5250665101721439\n",
            "f： 0.533833244876732\n",
            "epoch: 44\n",
            "train: 67.90178571428571 %\n",
            "准确率： 0.5386702245166656\n",
            "召回率： 0.5362128325508607\n",
            "f： 0.5374387194954989\n",
            "epoch: 45\n",
            "train: 68.75558035714286 %\n",
            "准确率： 0.5431368368447304\n",
            "召回率： 0.5449804381846636\n",
            "f： 0.5440570757042292\n",
            "epoch: 46\n",
            "train: 69.82142857142857 %\n",
            "准确率： 0.5408658289438846\n",
            "召回率： 0.539424882629108\n",
            "f： 0.540144394783009\n",
            "epoch: 47\n",
            "train: 70.06138392857143 %\n",
            "准确率： 0.5401596774684864\n",
            "召回率： 0.5391901408450704\n",
            "f： 0.5396744737088596\n",
            "epoch: 48\n",
            "train: 70.49107142857143 %\n",
            "准确率： 0.5367438075627825\n",
            "召回率： 0.5416275430359936\n",
            "f： 0.539174616552251\n",
            "epoch: 49\n",
            "train: 70.91517857142857 %\n",
            "准确率： 0.5519071451639942\n",
            "召回率： 0.548329420970266\n",
            "f： 0.5501124660861278\n",
            "epoch: 50\n",
            "train: 70.53571428571429 %\n",
            "准确率： 0.5268462259126111\n",
            "召回率： 0.5261032863849766\n",
            "f： 0.5264744940473657\n",
            "epoch: 51\n",
            "train: 70.22321428571429 %\n",
            "准确率： 0.538142202363768\n",
            "召回率： 0.5378325508607199\n",
            "f： 0.5379873320554116\n",
            "epoch: 52\n",
            "train: 71.65178571428571 %\n",
            "准确率： 0.5578226303360253\n",
            "召回率： 0.5560876369327074\n",
            "f： 0.556953782447382\n",
            "epoch: 53\n",
            "train: 72.23772321428571 %\n",
            "准确率： 0.5461242331105001\n",
            "召回率： 0.5532863849765258\n",
            "f： 0.5496819799895144\n",
            "epoch: 54\n",
            "train: 72.55022321428571 %\n",
            "准确率： 0.5514614367534946\n",
            "召回率： 0.5525743348982787\n",
            "f： 0.5520173249100575\n",
            "epoch: 55\n",
            "train: 72.57254464285714 %\n",
            "准确率： 0.5545831952327375\n",
            "召回率： 0.5455946791862285\n",
            "f： 0.550052218864842\n",
            "epoch: 56\n",
            "train: 72.9296875 %\n",
            "准确率： 0.5505488209795629\n",
            "召回率： 0.545453834115806\n",
            "f： 0.5479894850165458\n",
            "epoch: 57\n",
            "train: 72.97433035714286 %\n",
            "准确率： 0.5713375528964101\n",
            "召回率： 0.5533098591549296\n",
            "f： 0.5621792173005129\n",
            "epoch: 58\n",
            "train: 72.63392857142857 %\n",
            "准确率： 0.5638230477632873\n",
            "召回率： 0.5619131455399061\n",
            "f： 0.5628664765000352\n",
            "epoch: 59\n",
            "train: 73.38727678571429 %\n",
            "准确率： 0.5640926355239574\n",
            "召回率： 0.5642723004694835\n",
            "f： 0.5641824536930623\n",
            "epoch: 60\n",
            "train: 73.74441964285714 %\n",
            "准确率： 0.5513499887738547\n",
            "召回率： 0.5505046948356807\n",
            "f： 0.5509270175687987\n",
            "epoch: 61\n",
            "train: 73.95089285714286 %\n",
            "准确率： 0.5600186894484492\n",
            "召回率： 0.5658646322378716\n",
            "f： 0.5629264838499325\n",
            "epoch: 62\n",
            "train: 74.63727678571429 %\n",
            "准确率： 0.5749402849589462\n",
            "召回率： 0.5587402190923317\n",
            "f： 0.5667245041878907\n",
            "epoch: 63\n",
            "train: 73.95647321428571 %\n",
            "准确率： 0.5644485291591005\n",
            "召回率： 0.5503129890453834\n",
            "f： 0.5572911374697028\n",
            "epoch: 64\n",
            "train: 74.89955357142857 %\n",
            "准确率： 0.5559612792115733\n",
            "召回率： 0.5530203442879499\n",
            "f： 0.5544869121818872\n",
            "epoch: 65\n",
            "train: 75.23995535714286 %\n",
            "准确率： 0.5412644217842015\n",
            "召回率： 0.5471713615023474\n",
            "f： 0.5442018631658164\n",
            "epoch: 66\n",
            "train: 75.22879464285714 %\n",
            "准确率： 0.5599525652055629\n",
            "召回率： 0.5554147104851331\n",
            "f： 0.557674406749048\n",
            "epoch: 67\n",
            "train: 75.55245535714286 %\n",
            "准确率： 0.5662281854065684\n",
            "召回率： 0.548141627543036\n",
            "f： 0.5570381313326771\n",
            "epoch: 68\n",
            "train: 76.06584821428571 %\n",
            "准确率： 0.5650274725099305\n",
            "召回率： 0.5597965571205007\n",
            "f： 0.562399851812341\n",
            "epoch: 69\n",
            "train: 76.49553571428571 %\n",
            "准确率： 0.5614439360442625\n",
            "召回率： 0.5527738654147104\n",
            "f： 0.557075168489424\n",
            "epoch: 70\n",
            "train: 76.23883928571429 %\n",
            "准确率： 0.5683101302800359\n",
            "召回率： 0.5436658841940533\n",
            "f： 0.555714917324474\n",
            "epoch: 71\n",
            "train: 76.27790178571429 %\n",
            "准确率： 0.5728463641868434\n",
            "召回率： 0.5653364632237873\n",
            "f： 0.5690666379789924\n",
            "epoch: 72\n",
            "train: 76.84151785714286 %\n",
            "准确率： 0.5634369601557268\n",
            "召回率： 0.5544561815336464\n",
            "f： 0.5589104965627848\n",
            "epoch: 73\n",
            "train: 76.88616071428571 %\n",
            "准确率： 0.5664890588888997\n",
            "召回率： 0.5651369327073552\n",
            "f： 0.5658121880024314\n",
            "epoch: 74\n",
            "train: 77.23214285714286 %\n",
            "准确率： 0.5722307020027513\n",
            "召回率： 0.5618192488262911\n",
            "f： 0.5669771828295622\n",
            "epoch: 75\n",
            "train: 76.88616071428571 %\n",
            "准确率： 0.5670449419068321\n",
            "召回率： 0.5585602503912362\n",
            "f： 0.5627706178005832\n",
            "epoch: 76\n",
            "train: 76.22767857142857 %\n",
            "准确率： 0.5558121289969032\n",
            "召回率： 0.5434233176838811\n",
            "f： 0.5495479099777439\n",
            "epoch: 77\n",
            "train: 75.90401785714286 %\n",
            "准确率： 0.5593945543474032\n",
            "召回率： 0.5524021909233178\n",
            "f： 0.5558763842879119\n",
            "epoch: 78\n",
            "train: 77.05357142857143 %\n",
            "准确率： 0.5604322549712296\n",
            "召回率： 0.5553521126760563\n",
            "f： 0.557880618934186\n",
            "epoch: 79\n",
            "train: 77.39397321428571 %\n",
            "准确率： 0.5696016307941019\n",
            "召回率： 0.5636541471048514\n",
            "f： 0.5666122823393345\n",
            "epoch: 80\n",
            "train: 77.67299107142857 %\n",
            "准确率： 0.5731028130177006\n",
            "召回率： 0.5674647887323944\n",
            "f： 0.5702698660070953\n",
            "epoch: 81\n",
            "train: 78.11383928571429 %\n",
            "准确率： 0.5739763415645177\n",
            "召回率： 0.5661032863849765\n",
            "f： 0.5700126294709895\n",
            "epoch: 82\n",
            "train: 78.77790178571429 %\n",
            "准确率： 0.5651331513062794\n",
            "召回率： 0.5611541471048512\n",
            "f： 0.5631366205928582\n",
            "epoch: 83\n",
            "train: 78.61049107142857 %\n",
            "准确率： 0.5721571801619504\n",
            "召回率： 0.5646283255086072\n",
            "f： 0.5683678212839403\n",
            "epoch: 84\n",
            "train: 78.4375 %\n",
            "准确率： 0.5627308770633295\n",
            "召回率： 0.5399021909233177\n",
            "f： 0.5510802138039469\n",
            "epoch: 85\n",
            "train: 77.62834821428571 %\n",
            "准确率： 0.5669502352493972\n",
            "召回率： 0.5520148669796557\n",
            "f： 0.5593828763235484\n",
            "epoch: 86\n",
            "train: 77.79575892857143 %\n",
            "准确率： 0.576804553282794\n",
            "召回率： 0.562140062597809\n",
            "f： 0.5693779015556312\n",
            "epoch: 87\n",
            "train: 78.26450892857143 %\n",
            "准确率： 0.5781069041810339\n",
            "召回率： 0.5828403755868545\n",
            "f： 0.5804639901126151\n",
            "epoch: 88\n",
            "train: 79.35267857142857 %\n",
            "准确率： 0.5768037016710305\n",
            "召回率： 0.5676643192488263\n",
            "f： 0.5721975182602633\n",
            "epoch: 89\n",
            "train: 79.38058035714286 %\n",
            "准确率： 0.5757804265687506\n",
            "召回率： 0.570320813771518\n",
            "f： 0.5730376163573617\n",
            "epoch: 90\n",
            "train: 79.82142857142857 %\n",
            "准确率： 0.5700164784601397\n",
            "召回率： 0.5640884194053207\n",
            "f： 0.567036955707975\n",
            "epoch: 91\n",
            "train: 79.58705357142857 %\n",
            "准确率： 0.5724660272487053\n",
            "召回率： 0.5678364632237872\n",
            "f： 0.5701418473513693\n",
            "epoch: 92\n",
            "train: 79.70424107142857 %\n",
            "准确率： 0.5704342024421621\n",
            "召回率： 0.5672496087636932\n",
            "f： 0.5688374484607951\n",
            "epoch: 93\n",
            "train: 79.44196428571429 %\n",
            "准确率： 0.5714145391844425\n",
            "召回率： 0.5579460093896714\n",
            "f： 0.5645999627802217\n",
            "epoch: 94\n",
            "train: 79.73772321428571 %\n",
            "准确率： 0.5699795679948462\n",
            "召回率： 0.5623435054773083\n",
            "f： 0.5661357890267273\n",
            "epoch: 95\n",
            "train: 80.11160714285714 %\n",
            "准确率： 0.5670241769093791\n",
            "召回率： 0.5558802816901408\n",
            "f： 0.5613969323420775\n",
            "epoch: 96\n",
            "train: 80.02790178571429 %\n",
            "准确率： 0.5796708732527128\n",
            "召回率： 0.5738106416275429\n",
            "f： 0.5767258710660258\n",
            "epoch: 97\n",
            "train: 80.27901785714286 %\n",
            "准确率： 0.5673504474360179\n",
            "召回率： 0.5677895148669797\n",
            "f： 0.5675698962367679\n",
            "epoch: 98\n",
            "train: 80.00558035714286 %\n",
            "准确率： 0.5688647284655058\n",
            "召回率： 0.5479107981220658\n",
            "f： 0.5581911852051795\n",
            "epoch: 99\n",
            "train: 80.13392857142857 %\n",
            "准确率： 0.5709126514098289\n",
            "召回率： 0.562621283255086\n",
            "f： 0.5667366432355005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X51hVgn0voWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "13a56403-1516-452b-b7fe-9106d8fc3b7d"
      },
      "source": [
        "torch.save(model, \"model_biLSTMATT_01.pkl\")\n",
        "print(\"model has been saved\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model has been saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BiLSTM_ATT. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmBPJzsaFKvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}